From 286df3692ce939479f64dc61436937a62fe43f03 Mon Sep 17 00:00:00 2001
From: Bo Gan <bg2539@bug01.cs.columbia.edu>
Date: Thu, 28 Jan 2016 02:37:35 -0500
Subject: [PATCH 1/3] disable SSE3 optimization & disable force inline

---
 lib/sanitizer_common/sanitizer_internal_defs.h |  2 +-
 lib/tsan/rtl/tsan_rtl.cc                       | 10 ++++++++++
 2 files changed, 11 insertions(+), 1 deletion(-)

diff --git a/lib/sanitizer_common/sanitizer_internal_defs.h b/lib/sanitizer_common/sanitizer_internal_defs.h
index b76c602..e54171b 100644
--- a/lib/sanitizer_common/sanitizer_internal_defs.h
+++ b/lib/sanitizer_common/sanitizer_internal_defs.h
@@ -134,7 +134,7 @@ using namespace __sanitizer;  // NOLINT
 # define UNLIKELY(x) (x)
 # define PREFETCH(x) /* _mm_prefetch(x, _MM_HINT_NTA) */
 #else  // _MSC_VER
-# define ALWAYS_INLINE inline __attribute__((always_inline))
+# define ALWAYS_INLINE inline //__attribute__((always_inline))
 # define ALIAS(x) __attribute__((alias(x)))
 // Please only use the ALIGNED macro before the type.
 // Using ALIGNED after the variable declaration is not portable!
diff --git a/lib/tsan/rtl/tsan_rtl.cc b/lib/tsan/rtl/tsan_rtl.cc
index 63c356b..b34040a 100644
--- a/lib/tsan/rtl/tsan_rtl.cc
+++ b/lib/tsan/rtl/tsan_rtl.cc
@@ -655,6 +655,11 @@ bool ContainsSameAccessSlow(u64 *s, u64 a, u64 sync_epoch, bool is_write) {
   return false;
 }
 
+#ifdef __SSE3__
+#define __NO_SSE3__
+#undef __SSE3__
+#endif
+
 #if defined(__SSE3__)
 #define SHUF(v0, v1, i0, i1, i2, i3) _mm_castps_si128(_mm_shuffle_ps( \
     _mm_castsi128_ps(v0), _mm_castsi128_ps(v1), \
@@ -727,6 +732,11 @@ bool ContainsSameAccess(u64 *s, u64 a, u64 sync_epoch, bool is_write) {
 #endif
 }
 
+#ifdef __NO_SSE3__
+#define __SSE3__
+#undef __NO_SSE3__
+#endif
+
 ALWAYS_INLINE USED
 void MemoryAccess(ThreadState *thr, uptr pc, uptr addr,
     int kAccessSizeLog, bool kAccessIsWrite, bool kIsAtomic) {
-- 
1.9.1


From d78bc17555a860ebb7c3d59fa062b97c751aa518 Mon Sep 17 00:00:00 2001
From: Bo Gan <bg2539@bug01.cs.columbia.edu>
Date: Thu, 28 Jan 2016 02:53:00 -0500
Subject: [PATCH 2/3] manually expand macro, for easier debugging

---
 lib/tsan/rtl/tsan_rtl.cc | 53 +++++++++++++++++++++++++++++++++++++++++++++++-
 1 file changed, 52 insertions(+), 1 deletion(-)

diff --git a/lib/tsan/rtl/tsan_rtl.cc b/lib/tsan/rtl/tsan_rtl.cc
index b34040a..d494aac 100644
--- a/lib/tsan/rtl/tsan_rtl.cc
+++ b/lib/tsan/rtl/tsan_rtl.cc
@@ -593,7 +593,58 @@ void MemoryAccessImpl1(ThreadState *thr, uptr addr,
   // threads, which is not enough for the unrolled loop.
 #if SANITIZER_DEBUG
   for (int idx = 0; idx < 4; idx++) {
-#include "tsan_update_shadow_word_inl.h"
+do {
+  StatInc(thr, StatShadowProcessed);
+  const unsigned kAccessSize = 1 << kAccessSizeLog;
+  u64 *sp = &shadow_mem[idx];
+  old = LoadShadow(sp);
+  if (old.IsZero()) {
+    StatInc(thr, StatShadowZero);
+    if (store_word)
+      StoreIfNotYetStored(sp, &store_word);
+    // The above StoreIfNotYetStored could be done unconditionally
+    // and it even shows 4% gain on synthetic benchmarks (r4307).
+    break;
+  }
+  // is the memory access equal to the previous?
+  if (Shadow::Addr0AndSizeAreEqual(cur, old)) {
+    StatInc(thr, StatShadowSameSize);
+    // same thread?
+    if (Shadow::TidsAreEqual(old, cur)) {
+      StatInc(thr, StatShadowSameThread);
+      if (old.IsRWWeakerOrEqual(kAccessIsWrite, kIsAtomic))
+        StoreIfNotYetStored(sp, &store_word);
+      break;
+    }
+    StatInc(thr, StatShadowAnotherThread);
+    if (HappensBefore(old, thr)) {
+      if (old.IsRWWeakerOrEqual(kAccessIsWrite, kIsAtomic))
+        StoreIfNotYetStored(sp, &store_word);
+      break;
+    }
+    if (old.IsBothReadsOrAtomic(kAccessIsWrite, kIsAtomic))
+      break;
+    goto RACE;
+  }
+  // Do the memory access intersect?
+  if (Shadow::TwoRangesIntersect(old, cur, kAccessSize)) {
+    StatInc(thr, StatShadowIntersect);
+    if (Shadow::TidsAreEqual(old, cur)) {
+      StatInc(thr, StatShadowSameThread);
+      break;
+    }
+    StatInc(thr, StatShadowAnotherThread);
+    if (old.IsBothReadsOrAtomic(kAccessIsWrite, kIsAtomic))
+      break;
+    if (HappensBefore(old, thr))
+      break;
+    goto RACE;
+  }
+  // The accesses do not intersect.
+  StatInc(thr, StatShadowNotIntersect);
+  break;
+} while (0);
+//#include "tsan_update_shadow_word_inl.h"
   }
 #else
   int idx = 0;
-- 
1.9.1


From 44363a0cdf706551f999286f1ed8995587168c44 Mon Sep 17 00:00:00 2001
From: Bo Gan <bg2539@bug01.cs.columbia.edu>
Date: Mon, 1 Feb 2016 16:30:52 -0500
Subject: [PATCH 3/3] Implementing WriteMask

---
 lib/tsan/CMakeLists.txt             |   8 +-
 lib/tsan/rtl/compiler.h             | 112 +++++++
 lib/tsan/rtl/rbtree.cc              | 586 ++++++++++++++++++++++++++++++++++++
 lib/tsan/rtl/rbtree.h               | 114 +++++++
 lib/tsan/rtl/rbtree_augmented.h     | 249 +++++++++++++++
 lib/tsan/rtl/tsan_fd.cc             |   2 +-
 lib/tsan/rtl/tsan_interceptors.cc   |   4 +-
 lib/tsan/rtl/tsan_interface.cc      |  42 +--
 lib/tsan/rtl/tsan_interface.h       |  28 +-
 lib/tsan/rtl/tsan_interface_inl.h   |  34 +--
 lib/tsan/rtl/tsan_platform.h        |   2 +
 lib/tsan/rtl/tsan_platform_linux.cc |   9 +-
 lib/tsan/rtl/tsan_rtl.cc            |  37 ++-
 lib/tsan/rtl/tsan_rtl.h             |  62 +++-
 lib/tsan/rtl/tsan_rtl_mutex.cc      |   4 +-
 lib/tsan/rtl/tsan_rtl_report.cc     |  37 +++
 16 files changed, 1260 insertions(+), 70 deletions(-)
 create mode 100644 lib/tsan/rtl/compiler.h
 create mode 100644 lib/tsan/rtl/rbtree.cc
 create mode 100644 lib/tsan/rtl/rbtree.h
 create mode 100644 lib/tsan/rtl/rbtree_augmented.h

diff --git a/lib/tsan/CMakeLists.txt b/lib/tsan/CMakeLists.txt
index 9013780..e54eba8 100644
--- a/lib/tsan/CMakeLists.txt
+++ b/lib/tsan/CMakeLists.txt
@@ -39,7 +39,8 @@ set(TSAN_SOURCES
   rtl/tsan_stat.cc
   rtl/tsan_suppressions.cc
   rtl/tsan_symbolize.cc
-  rtl/tsan_sync.cc)
+  rtl/tsan_sync.cc
+  rtl/rbtree.cc)
 
 set(TSAN_CXX_SOURCES
   rtl/tsan_new_delete.cc)
@@ -78,7 +79,10 @@ set(TSAN_HEADERS
   rtl/tsan_sync.h
   rtl/tsan_trace.h
   rtl/tsan_update_shadow_word_inl.h
-  rtl/tsan_vector.h)
+  rtl/tsan_vector.h
+  rtl/rbtree.h
+  rtl/rbtree_augmented.h
+  rtl/compiler.h)
 
 set(TSAN_RUNTIME_LIBRARIES)
 add_custom_target(tsan)
diff --git a/lib/tsan/rtl/compiler.h b/lib/tsan/rtl/compiler.h
new file mode 100644
index 0000000..4b045dc
--- /dev/null
+++ b/lib/tsan/rtl/compiler.h
@@ -0,0 +1,112 @@
+#ifndef __always_inline
+#define __always_inline inline
+#endif
+
+#ifndef barrier
+#define barrier() __asm__ __volatile__("": : :"memory")
+#endif
+
+#ifndef __force
+#define __force
+#endif
+
+#define __READ_ONCE_SIZE						\
+({									\
+	switch (size) {							\
+	case 1: *(u8 *)res = *(volatile u8 *)p; break;		\
+	case 2: *(u16 *)res = *(volatile u16 *)p; break;		\
+	case 4: *(u32 *)res = *(volatile u32 *)p; break;		\
+	case 8: *(u64 *)res = *(volatile u64 *)p; break;		\
+	default:							\
+		barrier();						\
+		__builtin_memcpy((void *)res, (const void *)p, size);	\
+		barrier();						\
+	}								\
+})
+
+static __always_inline
+void __read_once_size(const volatile void *p, void *res, int size)
+{
+    __READ_ONCE_SIZE;
+}
+
+#ifdef CONFIG_KASAN
+/*
+ * This function is not 'inline' because __no_sanitize_address confilcts
+ * with inlining. Attempt to inline it may cause a build failure.
+ * 	https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368
+ * '__maybe_unused' allows us to avoid defined-but-not-used warnings.
+ */
+static __no_sanitize_address __maybe_unused
+void __read_once_size_nocheck(const volatile void *p, void *res, int size)
+{
+	__READ_ONCE_SIZE;
+}
+#else
+static __always_inline
+void __read_once_size_nocheck(const volatile void *p, void *res, int size)
+{
+    __READ_ONCE_SIZE;
+}
+#endif
+
+static __always_inline void __write_once_size(volatile void *p, void *res, int size)
+{
+    switch (size) {
+        case 1: *(volatile u8 *)p = *(u8 *)res; break;
+        case 2: *(volatile u16 *)p = *(u16 *)res; break;
+        case 4: *(volatile u32 *)p = *(u32 *)res; break;
+        case 8: *(volatile u64 *)p = *(u64 *)res; break;
+        default:
+            barrier();
+            __builtin_memcpy((void *)p, (const void *)res, size);
+            barrier();
+    }
+}
+
+/*
+ * Prevent the compiler from merging or refetching reads or writes. The
+ * compiler is also forbidden from reordering successive instances of
+ * READ_ONCE, WRITE_ONCE and ACCESS_ONCE (see below), but only when the
+ * compiler is aware of some particular ordering.  One way to make the
+ * compiler aware of ordering is to put the two invocations of READ_ONCE,
+ * WRITE_ONCE or ACCESS_ONCE() in different C statements.
+ *
+ * In contrast to ACCESS_ONCE these two macros will also work on aggregate
+ * data types like structs or unions. If the size of the accessed data
+ * type exceeds the word size of the machine (e.g., 32 bits or 64 bits)
+ * READ_ONCE() and WRITE_ONCE()  will fall back to memcpy and print a
+ * compile-time warning.
+ *
+ * Their two major use cases are: (1) Mediating communication between
+ * process-level code and irq/NMI handlers, all running on the same CPU,
+ * and (2) Ensuring that the compiler does not  fold, spindle, or otherwise
+ * mutilate accesses that either do not require ordering or that interact
+ * with an explicit memory barrier or atomic instruction that provides the
+ * required ordering.
+ */
+
+#define __READ_ONCE(x, check)                                           \
+({                                                                      \
+        union { decltype(x) __val; char __c[1]; } __u;                    \
+        if (check)                                                      \
+                __read_once_size(&(x), __u.__c, sizeof(x));             \
+        else                                                            \
+                __read_once_size_nocheck(&(x), __u.__c, sizeof(x));     \
+        __u.__val;                                                      \
+})
+#define READ_ONCE(x) __READ_ONCE(x, 1)
+
+/*
+ * Use READ_ONCE_NOCHECK() instead of READ_ONCE() if you need
+ * to hide memory access from KASAN.
+ */
+#define READ_ONCE_NOCHECK(x) __READ_ONCE(x, 0)
+
+#define WRITE_ONCE(x, val) \
+({                                                      \
+        union { decltype(x) __val; char __c[1]; } __u =   \
+                { .__val = (__force decltype(x)) (val) }; \
+        __write_once_size(&(x), __u.__c, sizeof(x));    \
+        __u.__val;                                      \
+})
diff --git a/lib/tsan/rtl/rbtree.cc b/lib/tsan/rtl/rbtree.cc
new file mode 100644
index 0000000..8fe6bd7
--- /dev/null
+++ b/lib/tsan/rtl/rbtree.cc
@@ -0,0 +1,586 @@
+/*
+  Red Black Trees
+  (C) 1999  Andrea Arcangeli <andrea@suse.de>
+  (C) 2002  David Woodhouse <dwmw2@infradead.org>
+  (C) 2012  Michel Lespinasse <walken@google.com>
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of the GNU General Public License as published by
+  the Free Software Foundation; either version 2 of the License, or
+  (at your option) any later version.
+
+  This program is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with this program; if not, write to the Free Software
+  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+
+  linux/lib/rbtree.c
+*/
+
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wpedantic"
+#pragma GCC diagnostic ignored "-Wcast-qual"
+
+#include "rbtree_augmented.h"
+
+/*
+ * red-black trees properties:  http://en.wikipedia.org/wiki/Rbtree
+ *
+ *  1) A node is either red or black
+ *  2) The root is black
+ *  3) All leaves (NULL) are black
+ *  4) Both children of every red node are black
+ *  5) Every simple path from root to leaves contains the same number
+ *     of black nodes.
+ *
+ *  4 and 5 give the O(log n) guarantee, since 4 implies you cannot have two
+ *  consecutive red nodes in a path and every red node is therefore followed by
+ *  a black. So if B is the number of black nodes on every simple path (as per
+ *  5), then the longest possible path due to 4 is 2B.
+ *
+ *  We shall indicate color with case, where black nodes are uppercase and red
+ *  nodes will be lowercase. Unknown color nodes shall be drawn as red within
+ *  parentheses and have some accompanying text comment.
+ */
+
+/*
+ * Notes on lockless lookups:
+ *
+ * All stores to the tree structure (rb_left and rb_right) must be done using
+ * WRITE_ONCE(). And we must not inadvertently cause (temporary) loops in the
+ * tree structure as seen in program order.
+ *
+ * These two requirements will allow lockless iteration of the tree -- not
+ * correct iteration mind you, tree rotations are not atomic so a lookup might
+ * miss entire subtrees.
+ *
+ * But they do guarantee that any such traversal will only see valid elements
+ * and that it will indeed complete -- does not get stuck in a loop.
+ *
+ * It also guarantees that if the lookup returns an element it is the 'correct'
+ * one. But not returning an element does _NOT_ mean it's not present.
+ *
+ * NOTE:
+ *
+ * Stores to __rb_parent_color are not important for simple lookups so those
+ * are left undone as of now. Nor did I check for loops involving parent
+ * pointers.
+ */
+
+static inline void rb_set_black(struct rb_node *rb)
+{
+	rb->__rb_parent_color |= RB_BLACK;
+}
+
+static inline struct rb_node *rb_red_parent(struct rb_node *red)
+{
+	return (struct rb_node *)red->__rb_parent_color;
+}
+
+/*
+ * Helper function for rotations:
+ * - old's parent and color get assigned to new
+ * - old gets assigned new as a parent and 'color' as a color.
+ */
+static inline void
+__rb_rotate_set_parents(struct rb_node *old, struct rb_node *_new,
+			struct rb_root *root, int color)
+{
+	struct rb_node *parent = rb_parent(old);
+	_new->__rb_parent_color = old->__rb_parent_color;
+	rb_set_parent_color(old, _new, color);
+	__rb_change_child(old, _new, parent, root);
+}
+
+static inline void
+__rb_insert(struct rb_node *node, struct rb_root *root,
+	    void (*augment_rotate)(struct rb_node *old, struct rb_node *_new))
+{
+	struct rb_node *parent = rb_red_parent(node), *gparent, *tmp;
+
+	while (true) {
+		/*
+		 * Loop invariant: node is red
+		 *
+		 * If there is a black parent, we are done.
+		 * Otherwise, take some corrective action as we don't
+		 * want a red root or two consecutive red nodes.
+		 */
+		if (!parent) {
+			rb_set_parent_color(node, nullptr, RB_BLACK);
+			break;
+		} else if (rb_is_black(parent))
+			break;
+
+		gparent = rb_red_parent(parent);
+
+		tmp = gparent->rb_right;
+		if (parent != tmp) {	/* parent == gparent->rb_left */
+			if (tmp && rb_is_red(tmp)) {
+				/*
+				 * Case 1 - color flips
+				 *
+				 *       G            g
+				 *      / \          / \
+				 *     p   u  -->   P   U
+				 *    /            /
+				 *   n            n
+				 *
+				 * However, since g's parent might be red, and
+				 * 4) does not allow this, we need to recurse
+				 * at g.
+				 */
+				rb_set_parent_color(tmp, gparent, RB_BLACK);
+				rb_set_parent_color(parent, gparent, RB_BLACK);
+				node = gparent;
+				parent = rb_parent(node);
+				rb_set_parent_color(node, parent, RB_RED);
+				continue;
+			}
+
+			tmp = parent->rb_right;
+			if (node == tmp) {
+				/*
+				 * Case 2 - left rotate at parent
+				 *
+				 *      G             G
+				 *     / \           / \
+				 *    p   U  -->    n   U
+				 *     \           /
+				 *      n         p
+				 *
+				 * This still leaves us in violation of 4), the
+				 * continuation into Case 3 will fix that.
+				 */
+				tmp = node->rb_left;
+				WRITE_ONCE(parent->rb_right, tmp);
+				WRITE_ONCE(node->rb_left, parent);
+				if (tmp)
+					rb_set_parent_color(tmp, parent,
+							    RB_BLACK);
+				rb_set_parent_color(parent, node, RB_RED);
+				augment_rotate(parent, node);
+				parent = node;
+				tmp = node->rb_right;
+			}
+
+			/*
+			 * Case 3 - right rotate at gparent
+			 *
+			 *        G           P
+			 *       / \         / \
+			 *      p   U  -->  n   g
+			 *     /                 \
+			 *    n                   U
+			 */
+			WRITE_ONCE(gparent->rb_left, tmp); /* == parent->rb_right */
+			WRITE_ONCE(parent->rb_right, gparent);
+			if (tmp)
+				rb_set_parent_color(tmp, gparent, RB_BLACK);
+			__rb_rotate_set_parents(gparent, parent, root, RB_RED);
+			augment_rotate(gparent, parent);
+			break;
+		} else {
+			tmp = gparent->rb_left;
+			if (tmp && rb_is_red(tmp)) {
+				/* Case 1 - color flips */
+				rb_set_parent_color(tmp, gparent, RB_BLACK);
+				rb_set_parent_color(parent, gparent, RB_BLACK);
+				node = gparent;
+				parent = rb_parent(node);
+				rb_set_parent_color(node, parent, RB_RED);
+				continue;
+			}
+
+			tmp = parent->rb_left;
+			if (node == tmp) {
+				/* Case 2 - right rotate at parent */
+				tmp = node->rb_right;
+				WRITE_ONCE(parent->rb_left, tmp);
+				WRITE_ONCE(node->rb_right, parent);
+				if (tmp)
+					rb_set_parent_color(tmp, parent,
+							    RB_BLACK);
+				rb_set_parent_color(parent, node, RB_RED);
+				augment_rotate(parent, node);
+				parent = node;
+				tmp = node->rb_left;
+			}
+
+			/* Case 3 - left rotate at gparent */
+			WRITE_ONCE(gparent->rb_right, tmp); /* == parent->rb_left */
+			WRITE_ONCE(parent->rb_left, gparent);
+			if (tmp)
+				rb_set_parent_color(tmp, gparent, RB_BLACK);
+			__rb_rotate_set_parents(gparent, parent, root, RB_RED);
+			augment_rotate(gparent, parent);
+			break;
+		}
+	}
+}
+
+/*
+ * Inline version for rb_erase() use - we want to be able to inline
+ * and eliminate the dummy_rotate callback there
+ */
+static __always_inline void
+____rb_erase_color(struct rb_node *parent, struct rb_root *root,
+	void (*augment_rotate)(struct rb_node *old, struct rb_node *_new))
+{
+	struct rb_node *node = nullptr, *sibling, *tmp1, *tmp2;
+
+	while (true) {
+		/*
+		 * Loop invariants:
+		 * - node is black (or NULL on first iteration)
+		 * - node is not the root (parent is not NULL)
+		 * - All leaf paths going through parent and node have a
+		 *   black node count that is 1 lower than other leaf paths.
+		 */
+		sibling = parent->rb_right;
+		if (node != sibling) {	/* node == parent->rb_left */
+			if (rb_is_red(sibling)) {
+				/*
+				 * Case 1 - left rotate at parent
+				 *
+				 *     P               S
+				 *    / \             / \
+				 *   N   s    -->    p   Sr
+				 *      / \         / \
+				 *     Sl  Sr      N   Sl
+				 */
+				tmp1 = sibling->rb_left;
+				WRITE_ONCE(parent->rb_right, tmp1);
+				WRITE_ONCE(sibling->rb_left, parent);
+				rb_set_parent_color(tmp1, parent, RB_BLACK);
+				__rb_rotate_set_parents(parent, sibling, root,
+							RB_RED);
+				augment_rotate(parent, sibling);
+				sibling = tmp1;
+			}
+			tmp1 = sibling->rb_right;
+			if (!tmp1 || rb_is_black(tmp1)) {
+				tmp2 = sibling->rb_left;
+				if (!tmp2 || rb_is_black(tmp2)) {
+					/*
+					 * Case 2 - sibling color flip
+					 * (p could be either color here)
+					 *
+					 *    (p)           (p)
+					 *    / \           / \
+					 *   N   S    -->  N   s
+					 *      / \           / \
+					 *     Sl  Sr        Sl  Sr
+					 *
+					 * This leaves us violating 5) which
+					 * can be fixed by flipping p to black
+					 * if it was red, or by recursing at p.
+					 * p is red when coming from Case 1.
+					 */
+					rb_set_parent_color(sibling, parent,
+							    RB_RED);
+					if (rb_is_red(parent))
+						rb_set_black(parent);
+					else {
+						node = parent;
+						parent = rb_parent(node);
+						if (parent)
+							continue;
+					}
+					break;
+				}
+				/*
+				 * Case 3 - right rotate at sibling
+				 * (p could be either color here)
+				 *
+				 *   (p)           (p)
+				 *   / \           / \
+				 *  N   S    -->  N   Sl
+				 *     / \             \
+				 *    sl  Sr            s
+				 *                       \
+				 *                        Sr
+				 */
+				tmp1 = tmp2->rb_right;
+				WRITE_ONCE(sibling->rb_left, tmp1);
+				WRITE_ONCE(tmp2->rb_right, sibling);
+				WRITE_ONCE(parent->rb_right, tmp2);
+				if (tmp1)
+					rb_set_parent_color(tmp1, sibling,
+							    RB_BLACK);
+				augment_rotate(sibling, tmp2);
+				tmp1 = sibling;
+				sibling = tmp2;
+			}
+			/*
+			 * Case 4 - left rotate at parent + color flips
+			 * (p and sl could be either color here.
+			 *  After rotation, p becomes black, s acquires
+			 *  p's color, and sl keeps its color)
+			 *
+			 *      (p)             (s)
+			 *      / \             / \
+			 *     N   S     -->   P   Sr
+			 *        / \         / \
+			 *      (sl) sr      N  (sl)
+			 */
+			tmp2 = sibling->rb_left;
+			WRITE_ONCE(parent->rb_right, tmp2);
+			WRITE_ONCE(sibling->rb_left, parent);
+			rb_set_parent_color(tmp1, sibling, RB_BLACK);
+			if (tmp2)
+				rb_set_parent(tmp2, parent);
+			__rb_rotate_set_parents(parent, sibling, root,
+						RB_BLACK);
+			augment_rotate(parent, sibling);
+			break;
+		} else {
+			sibling = parent->rb_left;
+			if (rb_is_red(sibling)) {
+				/* Case 1 - right rotate at parent */
+				tmp1 = sibling->rb_right;
+				WRITE_ONCE(parent->rb_left, tmp1);
+				WRITE_ONCE(sibling->rb_right, parent);
+				rb_set_parent_color(tmp1, parent, RB_BLACK);
+				__rb_rotate_set_parents(parent, sibling, root,
+							RB_RED);
+				augment_rotate(parent, sibling);
+				sibling = tmp1;
+			}
+			tmp1 = sibling->rb_left;
+			if (!tmp1 || rb_is_black(tmp1)) {
+				tmp2 = sibling->rb_right;
+				if (!tmp2 || rb_is_black(tmp2)) {
+					/* Case 2 - sibling color flip */
+					rb_set_parent_color(sibling, parent,
+							    RB_RED);
+					if (rb_is_red(parent))
+						rb_set_black(parent);
+					else {
+						node = parent;
+						parent = rb_parent(node);
+						if (parent)
+							continue;
+					}
+					break;
+				}
+				/* Case 3 - right rotate at sibling */
+				tmp1 = tmp2->rb_left;
+				WRITE_ONCE(sibling->rb_right, tmp1);
+				WRITE_ONCE(tmp2->rb_left, sibling);
+				WRITE_ONCE(parent->rb_left, tmp2);
+				if (tmp1)
+					rb_set_parent_color(tmp1, sibling,
+							    RB_BLACK);
+				augment_rotate(sibling, tmp2);
+				tmp1 = sibling;
+				sibling = tmp2;
+			}
+			/* Case 4 - left rotate at parent + color flips */
+			tmp2 = sibling->rb_right;
+			WRITE_ONCE(parent->rb_left, tmp2);
+			WRITE_ONCE(sibling->rb_right, parent);
+			rb_set_parent_color(tmp1, sibling, RB_BLACK);
+			if (tmp2)
+				rb_set_parent(tmp2, parent);
+			__rb_rotate_set_parents(parent, sibling, root,
+						RB_BLACK);
+			augment_rotate(parent, sibling);
+			break;
+		}
+	}
+}
+
+/* Non-inline version for rb_erase_augmented() use */
+void __rb_erase_color(struct rb_node *parent, struct rb_root *root,
+	void (*augment_rotate)(struct rb_node *old, struct rb_node *_new))
+{
+	____rb_erase_color(parent, root, augment_rotate);
+}
+
+/*
+ * Non-augmented rbtree manipulation functions.
+ *
+ * We use dummy augmented callbacks here, and have the compiler optimize them
+ * out of the rb_insert_color() and rb_erase() function definitions.
+ */
+
+static inline void dummy_propagate(struct rb_node *node, struct rb_node *stop) {}
+static inline void dummy_copy(struct rb_node *old, struct rb_node *_new) {}
+static inline void dummy_rotate(struct rb_node *old, struct rb_node *_new) {}
+
+static const struct rb_augment_callbacks dummy_callbacks = {
+	dummy_propagate, dummy_copy, dummy_rotate
+};
+
+void rb_insert_color(struct rb_node *node, struct rb_root *root)
+{
+	__rb_insert(node, root, dummy_rotate);
+}
+
+void rb_erase(struct rb_node *node, struct rb_root *root)
+{
+	struct rb_node *rebalance;
+	rebalance = __rb_erase_augmented(node, root, &dummy_callbacks);
+	if (rebalance)
+		____rb_erase_color(rebalance, root, dummy_rotate);
+}
+
+/*
+ * Augmented rbtree manipulation functions.
+ *
+ * This instantiates the same __always_inline functions as in the non-augmented
+ * case, but this time with user-defined callbacks.
+ */
+
+void __rb_insert_augmented(struct rb_node *node, struct rb_root *root,
+	void (*augment_rotate)(struct rb_node *old, struct rb_node *_new))
+{
+	__rb_insert(node, root, augment_rotate);
+}
+
+/*
+ * This function returns the first node (in sort order) of the tree.
+ */
+struct rb_node *rb_first(const struct rb_root *root)
+{
+	struct rb_node	*n;
+
+	n = root->rb_node;
+	if (!n)
+		return nullptr;
+	while (n->rb_left)
+		n = n->rb_left;
+	return n;
+}
+
+struct rb_node *rb_last(const struct rb_root *root)
+{
+	struct rb_node	*n;
+
+	n = root->rb_node;
+	if (!n)
+		return nullptr;
+	while (n->rb_right)
+		n = n->rb_right;
+	return n;
+}
+
+struct rb_node *rb_next(const struct rb_node *node)
+{
+	struct rb_node *parent;
+
+	if (RB_EMPTY_NODE(node))
+		return nullptr;
+
+	/*
+	 * If we have a right-hand child, go down and then left as far
+	 * as we can.
+	 */
+	if (node->rb_right) {
+		node = node->rb_right; 
+		while (node->rb_left)
+			node=node->rb_left;
+		return (struct rb_node *)node;
+	}
+
+	/*
+	 * No right-hand children. Everything down and left is smaller than us,
+	 * so any 'next' node must be in the general direction of our parent.
+	 * Go up the tree; any time the ancestor is a right-hand child of its
+	 * parent, keep going up. First time it's a left-hand child of its
+	 * parent, said parent is our 'next' node.
+	 */
+	while ((parent = rb_parent(node)) && node == parent->rb_right)
+		node = parent;
+
+	return parent;
+}
+
+struct rb_node *rb_prev(const struct rb_node *node)
+{
+	struct rb_node *parent;
+
+	if (RB_EMPTY_NODE(node))
+		return nullptr;
+
+	/*
+	 * If we have a left-hand child, go down and then right as far
+	 * as we can.
+	 */
+	if (node->rb_left) {
+		node = node->rb_left; 
+		while (node->rb_right)
+			node=node->rb_right;
+		return (struct rb_node *)node;
+	}
+
+	/*
+	 * No left-hand children. Go up till we find an ancestor which
+	 * is a right-hand child of its parent.
+	 */
+	while ((parent = rb_parent(node)) && node == parent->rb_left)
+		node = parent;
+
+	return parent;
+}
+
+void rb_replace_node(struct rb_node *victim, struct rb_node *_new,
+		     struct rb_root *root)
+{
+	struct rb_node *parent = rb_parent(victim);
+
+	/* Set the surrounding nodes to point to the replacement */
+	__rb_change_child(victim, _new, parent, root);
+	if (victim->rb_left)
+		rb_set_parent(victim->rb_left, _new);
+	if (victim->rb_right)
+		rb_set_parent(victim->rb_right, _new);
+
+	/* Copy the pointers/colour from the victim to the replacement */
+	*_new = *victim;
+}
+
+static struct rb_node *rb_left_deepest_node(const struct rb_node *node)
+{
+	for (;;) {
+		if (node->rb_left)
+			node = node->rb_left;
+		else if (node->rb_right)
+			node = node->rb_right;
+		else
+			return (struct rb_node *)node;
+	}
+}
+
+struct rb_node *rb_next_postorder(const struct rb_node *node)
+{
+	const struct rb_node *parent;
+	if (!node)
+		return nullptr;
+	parent = rb_parent(node);
+
+	/* If we're sitting on node, we've already seen our children */
+	if (parent && node == parent->rb_left && parent->rb_right) {
+		/* If we are the parent's left node, go to the parent's right
+		 * node then all the way down to the left */
+		return rb_left_deepest_node(parent->rb_right);
+	} else
+		/* Otherwise we are the parent's right node, and the parent
+		 * should be next */
+		return (struct rb_node *)parent;
+}
+
+struct rb_node *rb_first_postorder(const struct rb_root *root)
+{
+	if (!root->rb_node)
+		return nullptr;
+
+	return rb_left_deepest_node(root->rb_node);
+}
+
+#pragma GCC diagnostic pop
\ No newline at end of file
diff --git a/lib/tsan/rtl/rbtree.h b/lib/tsan/rtl/rbtree.h
new file mode 100644
index 0000000..db36189
--- /dev/null
+++ b/lib/tsan/rtl/rbtree.h
@@ -0,0 +1,114 @@
+/*
+  Red Black Trees
+  (C) 1999  Andrea Arcangeli <andrea@suse.de>
+  
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of the GNU General Public License as published by
+  the Free Software Foundation; either version 2 of the License, or
+  (at your option) any later version.
+
+  This program is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with this program; if not, write to the Free Software
+  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+
+  linux/include/linux/rbtree.h
+
+  To use rbtrees you'll have to implement your own insert and search cores.
+  This will avoid us to use callbacks and to drop drammatically performances.
+  I know it's not the cleaner way,  but in C (not in C++) to get
+  performances and genericity...
+
+  See Documentation/rbtree.txt for documentation and samples.
+*/
+
+#ifndef	_LINUX_RBTREE_H
+#define	_LINUX_RBTREE_H
+
+#include "sanitizer_common/sanitizer_internal_defs.h"
+
+struct rb_node {
+	unsigned long  __rb_parent_color;
+	struct rb_node *rb_right;
+	struct rb_node *rb_left;
+} __attribute__((aligned(sizeof(long))));
+    /* The alignment might seem pointless, but allegedly CRIS needs it */
+
+struct rb_root {
+	struct rb_node *rb_node;
+};
+
+#define rb_parent(r)   ((struct rb_node *)((r)->__rb_parent_color & ~3))
+
+#define RB_ROOT	(struct rb_root) { nullptr, }
+#define	rb_entry(ptr, type, member) container_of(ptr, type, member)
+
+#define RB_EMPTY_ROOT(root)  (READ_ONCE((root)->rb_node) == NULL)
+
+/* 'empty' nodes are nodes that are known not to be inserted in an rbtree */
+#define RB_EMPTY_NODE(node)  \
+	((node)->__rb_parent_color == (unsigned long)(node))
+#define RB_CLEAR_NODE(node)  \
+	((node)->__rb_parent_color = (unsigned long)(node))
+
+
+extern void rb_insert_color(struct rb_node *, struct rb_root *);
+extern void rb_erase(struct rb_node *, struct rb_root *);
+
+
+/* Find logical next and previous nodes in a tree */
+extern struct rb_node *rb_next(const struct rb_node *);
+extern struct rb_node *rb_prev(const struct rb_node *);
+extern struct rb_node *rb_first(const struct rb_root *);
+extern struct rb_node *rb_last(const struct rb_root *);
+
+/* Postorder iteration - always visit the parent after its children */
+extern struct rb_node *rb_first_postorder(const struct rb_root *);
+extern struct rb_node *rb_next_postorder(const struct rb_node *);
+
+/* Fast replacement of a single node without remove/rebalance/add/rebalance */
+extern void rb_replace_node(struct rb_node *victim, struct rb_node *_new,
+			    struct rb_root *root);
+
+static inline void rb_link_node(struct rb_node *node, struct rb_node *parent,
+				struct rb_node **rb_link)
+{
+	node->__rb_parent_color = (unsigned long)parent;
+	node->rb_left = node->rb_right = nullptr;
+
+	*rb_link = node;
+}
+
+#define rb_entry_safe(ptr, type, member) \
+	({ decltype(ptr) ____ptr = (ptr); \
+	   ____ptr ? rb_entry(____ptr, type, member) : NULL; \
+	})
+
+/**
+ * rbtree_postorder_for_each_entry_safe - iterate in post-order over rb_root of
+ * given type allowing the backing memory of @pos to be invalidated
+ *
+ * @pos:	the 'type *' to use as a loop cursor.
+ * @n:		another 'type *' to use as temporary storage
+ * @root:	'rb_root *' of the rbtree.
+ * @field:	the name of the rb_node field within 'type'.
+ *
+ * rbtree_postorder_for_each_entry_safe() provides a similar guarantee as
+ * list_for_each_entry_safe() and allows the iteration to continue independent
+ * of changes to @pos by the body of the loop.
+ *
+ * Note, however, that it cannot handle other modifications that re-order the
+ * rbtree it is iterating over. This includes calling rb_erase() on @pos, as
+ * rb_erase() may rebalance the tree, causing us to miss some nodes.
+ */
+#define rbtree_postorder_for_each_entry_safe(pos, n, root, field) \
+	for (pos = rb_entry_safe(rb_first_postorder(root), decltype(*pos), field); \
+	     pos && ({ n = rb_entry_safe(rb_next_postorder(&pos->field), \
+			decltype(*pos), field); 1; }); \
+	     pos = n)
+
+#endif	/* _LINUX_RBTREE_H */
diff --git a/lib/tsan/rtl/rbtree_augmented.h b/lib/tsan/rtl/rbtree_augmented.h
new file mode 100644
index 0000000..25818f5
--- /dev/null
+++ b/lib/tsan/rtl/rbtree_augmented.h
@@ -0,0 +1,249 @@
+/*
+  Red Black Trees
+  (C) 1999  Andrea Arcangeli <andrea@suse.de>
+  (C) 2002  David Woodhouse <dwmw2@infradead.org>
+  (C) 2012  Michel Lespinasse <walken@google.com>
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of the GNU General Public License as published by
+  the Free Software Foundation; either version 2 of the License, or
+  (at your option) any later version.
+
+  This program is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with this program; if not, write to the Free Software
+  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+
+  linux/include/linux/rbtree_augmented.h
+*/
+
+#ifndef _LINUX_RBTREE_AUGMENTED_H
+#define _LINUX_RBTREE_AUGMENTED_H
+
+#include "rbtree.h"
+#include "compiler.h"
+
+/*
+ * Please note - only struct rb_augment_callbacks and the prototypes for
+ * rb_insert_augmented() and rb_erase_augmented() are intended to be public.
+ * The rest are implementation details you are not expected to depend on.
+ *
+ * See Documentation/rbtree.txt for documentation and samples.
+ */
+
+struct rb_augment_callbacks {
+	void (*propagate)(struct rb_node *node, struct rb_node *stop);
+	void (*copy)(struct rb_node *old, struct rb_node *_new);
+	void (*rotate)(struct rb_node *old, struct rb_node *_new);
+};
+
+extern void __rb_insert_augmented(struct rb_node *node, struct rb_root *root,
+	void (*augment_rotate)(struct rb_node *old, struct rb_node *_new));
+/*
+ * Fixup the rbtree and update the augmented information when rebalancing.
+ *
+ * On insertion, the user must update the augmented information on the path
+ * leading to the inserted node, then call rb_link_node() as usual and
+ * rb_augment_inserted() instead of the usual rb_insert_color() call.
+ * If rb_augment_inserted() rebalances the rbtree, it will callback into
+ * a user provided function to update the augmented information on the
+ * affected subtrees.
+ */
+static inline void
+rb_insert_augmented(struct rb_node *node, struct rb_root *root,
+		    const struct rb_augment_callbacks *augment)
+{
+	__rb_insert_augmented(node, root, augment->rotate);
+}
+
+#define RB_DECLARE_CALLBACKS(rbstatic, rbname, rbstruct, rbfield,	\
+			     rbtype, rbaugmented, rbcompute)		\
+static inline void							\
+rbname ## _propagate(struct rb_node *rb, struct rb_node *stop)		\
+{									\
+	while (rb != stop) {						\
+		rbstruct *node = rb_entry(rb, rbstruct, rbfield);	\
+		rbtype augmented = rbcompute(node);			\
+		if (node->rbaugmented == augmented)			\
+			break;						\
+		node->rbaugmented = augmented;				\
+		rb = rb_parent(&node->rbfield);				\
+	}								\
+}									\
+static inline void							\
+rbname ## _copy(struct rb_node *rb_old, struct rb_node *rb_new)		\
+{									\
+	rbstruct *old = rb_entry(rb_old, rbstruct, rbfield);		\
+	rbstruct *new = rb_entry(rb_new, rbstruct, rbfield);		\
+	new->rbaugmented = old->rbaugmented;				\
+}									\
+static void								\
+rbname ## _rotate(struct rb_node *rb_old, struct rb_node *rb_new)	\
+{									\
+	rbstruct *old = rb_entry(rb_old, rbstruct, rbfield);		\
+	rbstruct *new = rb_entry(rb_new, rbstruct, rbfield);		\
+	new->rbaugmented = old->rbaugmented;				\
+	old->rbaugmented = rbcompute(old);				\
+}									\
+rbstatic const struct rb_augment_callbacks rbname = {			\
+	rbname ## _propagate, rbname ## _copy, rbname ## _rotate	\
+};
+
+
+#define	RB_RED		0
+#define	RB_BLACK	1
+
+#define __rb_parent(pc)    ((struct rb_node *)(pc & ~3))
+
+#define __rb_color(pc)     ((pc) & 1)
+#define __rb_is_black(pc)  __rb_color(pc)
+#define __rb_is_red(pc)    (!__rb_color(pc))
+#define rb_color(rb)       __rb_color((rb)->__rb_parent_color)
+#define rb_is_red(rb)      __rb_is_red((rb)->__rb_parent_color)
+#define rb_is_black(rb)    __rb_is_black((rb)->__rb_parent_color)
+
+static inline void rb_set_parent(struct rb_node *rb, struct rb_node *p)
+{
+	rb->__rb_parent_color = rb_color(rb) | (unsigned long)p;
+}
+
+static inline void rb_set_parent_color(struct rb_node *rb,
+				       struct rb_node *p, int color)
+{
+	rb->__rb_parent_color = (unsigned long)p | color;
+}
+
+static inline void
+__rb_change_child(struct rb_node *old, struct rb_node *_new,
+		  struct rb_node *parent, struct rb_root *root)
+{
+	if (parent) {
+		if (parent->rb_left == old)
+			WRITE_ONCE(parent->rb_left, _new);
+		else
+			WRITE_ONCE(parent->rb_right, _new);
+	} else
+		WRITE_ONCE(root->rb_node, _new);
+}
+
+extern void __rb_erase_color(struct rb_node *parent, struct rb_root *root,
+	void (*augment_rotate)(struct rb_node *old, struct rb_node *_new));
+
+static __always_inline struct rb_node *
+__rb_erase_augmented(struct rb_node *node, struct rb_root *root,
+		     const struct rb_augment_callbacks *augment)
+{
+	struct rb_node *child = node->rb_right;
+	struct rb_node *tmp = node->rb_left;
+	struct rb_node *parent, *rebalance;
+	unsigned long pc;
+
+	if (!tmp) {
+		/*
+		 * Case 1: node to erase has no more than 1 child (easy!)
+		 *
+		 * Note that if there is one child it must be red due to 5)
+		 * and node must be black due to 4). We adjust colors locally
+		 * so as to bypass __rb_erase_color() later on.
+		 */
+		pc = node->__rb_parent_color;
+		parent = __rb_parent(pc);
+		__rb_change_child(node, child, parent, root);
+		if (child) {
+			child->__rb_parent_color = pc;
+			rebalance = nullptr;
+		} else
+			rebalance = __rb_is_black(pc) ? parent : nullptr;
+		tmp = parent;
+	} else if (!child) {
+		/* Still case 1, but this time the child is node->rb_left */
+		tmp->__rb_parent_color = pc = node->__rb_parent_color;
+		parent = __rb_parent(pc);
+		__rb_change_child(node, tmp, parent, root);
+		rebalance = nullptr;
+		tmp = parent;
+	} else {
+		struct rb_node *successor = child, *child2;
+
+		tmp = child->rb_left;
+		if (!tmp) {
+			/*
+			 * Case 2: node's successor is its right child
+			 *
+			 *    (n)          (s)
+			 *    / \          / \
+			 *  (x) (s)  ->  (x) (c)
+			 *        \
+			 *        (c)
+			 */
+			parent = successor;
+			child2 = successor->rb_right;
+
+			augment->copy(node, successor);
+		} else {
+			/*
+			 * Case 3: node's successor is leftmost under
+			 * node's right child subtree
+			 *
+			 *    (n)          (s)
+			 *    / \          / \
+			 *  (x) (y)  ->  (x) (y)
+			 *      /            /
+			 *    (p)          (p)
+			 *    /            /
+			 *  (s)          (c)
+			 *    \
+			 *    (c)
+			 */
+			do {
+				parent = successor;
+				successor = tmp;
+				tmp = tmp->rb_left;
+			} while (tmp);
+			child2 = successor->rb_right;
+			WRITE_ONCE(parent->rb_left, child2);
+			WRITE_ONCE(successor->rb_right, child);
+			rb_set_parent(child, successor);
+
+			augment->copy(node, successor);
+			augment->propagate(parent, successor);
+		}
+
+		tmp = node->rb_left;
+		WRITE_ONCE(successor->rb_left, tmp);
+		rb_set_parent(tmp, successor);
+
+		pc = node->__rb_parent_color;
+		tmp = __rb_parent(pc);
+		__rb_change_child(node, successor, tmp, root);
+
+		if (child2) {
+			successor->__rb_parent_color = pc;
+			rb_set_parent_color(child2, parent, RB_BLACK);
+			rebalance = nullptr;
+		} else {
+			unsigned long pc2 = successor->__rb_parent_color;
+			successor->__rb_parent_color = pc;
+			rebalance = __rb_is_black(pc2) ? parent : nullptr;
+		}
+		tmp = successor;
+	}
+
+	augment->propagate(tmp, nullptr);
+	return rebalance;
+}
+
+static __always_inline void
+rb_erase_augmented(struct rb_node *node, struct rb_root *root,
+		   const struct rb_augment_callbacks *augment)
+{
+	struct rb_node *rebalance = __rb_erase_augmented(node, root, augment);
+	if (rebalance)
+		__rb_erase_color(rebalance, root, augment->rotate);
+}
+
+#endif	/* _LINUX_RBTREE_AUGMENTED_H */
diff --git a/lib/tsan/rtl/tsan_fd.cc b/lib/tsan/rtl/tsan_fd.cc
index d84df4a..3721b60 100644
--- a/lib/tsan/rtl/tsan_fd.cc
+++ b/lib/tsan/rtl/tsan_fd.cc
@@ -194,7 +194,7 @@ void FdClose(ThreadState *thr, uptr pc, int fd, bool write) {
   FdDesc *d = fddesc(thr, pc, fd);
   if (write) {
     // To catch races between fd usage and close.
-    MemoryWrite(thr, pc, (uptr)d, kSizeLog8);
+    MemoryWrite(thr, pc, (uptr)d, -1, kSizeLog8);
   } else {
     // This path is used only by dup2/dup3 calls.
     // We do read instead of write because there is a number of legitimate
diff --git a/lib/tsan/rtl/tsan_interceptors.cc b/lib/tsan/rtl/tsan_interceptors.cc
index b1a7ae6..9ba1f19 100644
--- a/lib/tsan/rtl/tsan_interceptors.cc
+++ b/lib/tsan/rtl/tsan_interceptors.cc
@@ -1227,14 +1227,14 @@ TSAN_INTERCEPTOR(int, pthread_rwlock_unlock, void *m) {
 
 TSAN_INTERCEPTOR(int, pthread_barrier_init, void *b, void *a, unsigned count) {
   SCOPED_TSAN_INTERCEPTOR(pthread_barrier_init, b, a, count);
-  MemoryWrite(thr, pc, (uptr)b, kSizeLog1);
+  MemoryWrite(thr, pc, (uptr)b, -1, kSizeLog1);
   int res = REAL(pthread_barrier_init)(b, a, count);
   return res;
 }
 
 TSAN_INTERCEPTOR(int, pthread_barrier_destroy, void *b) {
   SCOPED_TSAN_INTERCEPTOR(pthread_barrier_destroy, b);
-  MemoryWrite(thr, pc, (uptr)b, kSizeLog1);
+  MemoryWrite(thr, pc, (uptr)b, -1, kSizeLog1);
   int res = REAL(pthread_barrier_destroy)(b);
   return res;
 }
diff --git a/lib/tsan/rtl/tsan_interface.cc b/lib/tsan/rtl/tsan_interface.cc
index 809d2ab..ddc5108 100644
--- a/lib/tsan/rtl/tsan_interface.cc
+++ b/lib/tsan/rtl/tsan_interface.cc
@@ -33,9 +33,9 @@ void __tsan_read16(void *addr) {
   MemoryRead(cur_thread(), CALLERPC, (uptr)addr + 8, kSizeLog8);
 }
 
-void __tsan_write16(void *addr) {
-  MemoryWrite(cur_thread(), CALLERPC, (uptr)addr, kSizeLog8);
-  MemoryWrite(cur_thread(), CALLERPC, (uptr)addr + 8, kSizeLog8);
+void __tsan_write16(void *addr, uptr val) {
+  MemoryWrite(cur_thread(), CALLERPC, (uptr)addr, val, kSizeLog8);
+  MemoryWrite(cur_thread(), CALLERPC, (uptr)addr + 8, 0, kSizeLog8);
 }
 
 void __tsan_read16_pc(void *addr, void *pc) {
@@ -43,43 +43,43 @@ void __tsan_read16_pc(void *addr, void *pc) {
   MemoryRead(cur_thread(), (uptr)pc, (uptr)addr + 8, kSizeLog8);
 }
 
-void __tsan_write16_pc(void *addr, void *pc) {
-  MemoryWrite(cur_thread(), (uptr)pc, (uptr)addr, kSizeLog8);
-  MemoryWrite(cur_thread(), (uptr)pc, (uptr)addr + 8, kSizeLog8);
+void __tsan_write16_pc(void *addr, uptr val, void *pc) {
+  MemoryWrite(cur_thread(), (uptr)pc, (uptr)addr, val, kSizeLog8);
+  MemoryWrite(cur_thread(), (uptr)pc, (uptr)addr + 8, 0, kSizeLog8);
 }
 
 // __tsan_unaligned_read/write calls are emitted by compiler.
 
 void __tsan_unaligned_read2(const void *addr) {
-  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, 2, false, false);
+  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, 0, 2, false, false);
 }
 
 void __tsan_unaligned_read4(const void *addr) {
-  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, 4, false, false);
+  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, 0, 4, false, false);
 }
 
 void __tsan_unaligned_read8(const void *addr) {
-  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, 8, false, false);
+  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, 0, 8, false, false);
 }
 
 void __tsan_unaligned_read16(const void *addr) {
-  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, 16, false, false);
+  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, 0, 16, false, false);
 }
 
-void __tsan_unaligned_write2(void *addr) {
-  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, 2, true, false);
+void __tsan_unaligned_write2(void *addr, uptr val) {
+  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, val, 2, true, false);
 }
 
-void __tsan_unaligned_write4(void *addr) {
-  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, 4, true, false);
+void __tsan_unaligned_write4(void *addr, uptr val) {
+  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, val, 4, true, false);
 }
 
-void __tsan_unaligned_write8(void *addr) {
-  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, 8, true, false);
+void __tsan_unaligned_write8(void *addr, uptr val) {
+  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, val, 8, true, false);
 }
 
-void __tsan_unaligned_write16(void *addr) {
-  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, 16, true, false);
+void __tsan_unaligned_write16(void *addr, uptr val) {
+  UnalignedMemoryAccess(cur_thread(), CALLERPC, (uptr)addr, val, 16, true, false);
 }
 
 // __sanitizer_unaligned_load/store are for user instrumentation.
@@ -105,19 +105,19 @@ u64 __sanitizer_unaligned_load64(const uu64 *addr) {
 
 SANITIZER_INTERFACE_ATTRIBUTE
 void __sanitizer_unaligned_store16(uu16 *addr, u16 v) {
-  __tsan_unaligned_write2(addr);
+  __tsan_unaligned_write2(addr, v);
   *addr = v;
 }
 
 SANITIZER_INTERFACE_ATTRIBUTE
 void __sanitizer_unaligned_store32(uu32 *addr, u32 v) {
-  __tsan_unaligned_write4(addr);
+  __tsan_unaligned_write4(addr, v);
   *addr = v;
 }
 
 SANITIZER_INTERFACE_ATTRIBUTE
 void __sanitizer_unaligned_store64(uu64 *addr, u64 v) {
-  __tsan_unaligned_write8(addr);
+  __tsan_unaligned_write8(addr, v);
   *addr = v;
 }
 }  // extern "C"
diff --git a/lib/tsan/rtl/tsan_interface.h b/lib/tsan/rtl/tsan_interface.h
index 41e084b..108282f 100644
--- a/lib/tsan/rtl/tsan_interface.h
+++ b/lib/tsan/rtl/tsan_interface.h
@@ -35,21 +35,21 @@ SANITIZER_INTERFACE_ATTRIBUTE void __tsan_read4(void *addr);
 SANITIZER_INTERFACE_ATTRIBUTE void __tsan_read8(void *addr);
 SANITIZER_INTERFACE_ATTRIBUTE void __tsan_read16(void *addr);
 
-SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write1(void *addr);
-SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write2(void *addr);
-SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write4(void *addr);
-SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write8(void *addr);
-SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write16(void *addr);
+SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write1(void *addr, uptr val);
+SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write2(void *addr, uptr val);
+SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write4(void *addr, uptr val);
+SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write8(void *addr, uptr val);
+SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write16(void *addr, uptr val); //FIXME: hack not implemented
 
 SANITIZER_INTERFACE_ATTRIBUTE void __tsan_unaligned_read2(const void *addr);
 SANITIZER_INTERFACE_ATTRIBUTE void __tsan_unaligned_read4(const void *addr);
 SANITIZER_INTERFACE_ATTRIBUTE void __tsan_unaligned_read8(const void *addr);
 SANITIZER_INTERFACE_ATTRIBUTE void __tsan_unaligned_read16(const void *addr);
 
-SANITIZER_INTERFACE_ATTRIBUTE void __tsan_unaligned_write2(void *addr);
-SANITIZER_INTERFACE_ATTRIBUTE void __tsan_unaligned_write4(void *addr);
-SANITIZER_INTERFACE_ATTRIBUTE void __tsan_unaligned_write8(void *addr);
-SANITIZER_INTERFACE_ATTRIBUTE void __tsan_unaligned_write16(void *addr);
+SANITIZER_INTERFACE_ATTRIBUTE void __tsan_unaligned_write2(void *addr, uptr val);
+SANITIZER_INTERFACE_ATTRIBUTE void __tsan_unaligned_write4(void *addr, uptr val);
+SANITIZER_INTERFACE_ATTRIBUTE void __tsan_unaligned_write8(void *addr, uptr val);
+SANITIZER_INTERFACE_ATTRIBUTE void __tsan_unaligned_write16(void *addr, uptr val); //FIXME: hack not implemented
 
 SANITIZER_INTERFACE_ATTRIBUTE void __tsan_read1_pc(void *addr, void *pc);
 SANITIZER_INTERFACE_ATTRIBUTE void __tsan_read2_pc(void *addr, void *pc);
@@ -57,11 +57,11 @@ SANITIZER_INTERFACE_ATTRIBUTE void __tsan_read4_pc(void *addr, void *pc);
 SANITIZER_INTERFACE_ATTRIBUTE void __tsan_read8_pc(void *addr, void *pc);
 SANITIZER_INTERFACE_ATTRIBUTE void __tsan_read16_pc(void *addr, void *pc);
 
-SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write1_pc(void *addr, void *pc);
-SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write2_pc(void *addr, void *pc);
-SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write4_pc(void *addr, void *pc);
-SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write8_pc(void *addr, void *pc);
-SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write16_pc(void *addr, void *pc);
+SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write1_pc(void *addr, uptr val, void *pc);
+SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write2_pc(void *addr, uptr val, void *pc);
+SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write4_pc(void *addr, uptr val, void *pc);
+SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write8_pc(void *addr, uptr val, void *pc);
+SANITIZER_INTERFACE_ATTRIBUTE void __tsan_write16_pc(void *addr, uptr val, void *pc); //FIXME: hack not implemented
 
 SANITIZER_INTERFACE_ATTRIBUTE void __tsan_vptr_read(void **vptr_p);
 SANITIZER_INTERFACE_ATTRIBUTE
diff --git a/lib/tsan/rtl/tsan_interface_inl.h b/lib/tsan/rtl/tsan_interface_inl.h
index 8852aa3..3f273bf 100644
--- a/lib/tsan/rtl/tsan_interface_inl.h
+++ b/lib/tsan/rtl/tsan_interface_inl.h
@@ -34,20 +34,20 @@ void __tsan_read8(void *addr) {
   MemoryRead(cur_thread(), CALLERPC, (uptr)addr, kSizeLog8);
 }
 
-void __tsan_write1(void *addr) {
-  MemoryWrite(cur_thread(), CALLERPC, (uptr)addr, kSizeLog1);
+void __tsan_write1(void *addr, uptr val) {
+  MemoryWrite(cur_thread(), CALLERPC, (uptr)addr, val, kSizeLog1);
 }
 
-void __tsan_write2(void *addr) {
-  MemoryWrite(cur_thread(), CALLERPC, (uptr)addr, kSizeLog2);
+void __tsan_write2(void *addr, uptr val) {
+  MemoryWrite(cur_thread(), CALLERPC, (uptr)addr, val, kSizeLog2);
 }
 
-void __tsan_write4(void *addr) {
-  MemoryWrite(cur_thread(), CALLERPC, (uptr)addr, kSizeLog4);
+void __tsan_write4(void *addr, uptr val) {
+  MemoryWrite(cur_thread(), CALLERPC, (uptr)addr, val, kSizeLog4);
 }
 
-void __tsan_write8(void *addr) {
-  MemoryWrite(cur_thread(), CALLERPC, (uptr)addr, kSizeLog8);
+void __tsan_write8(void *addr, uptr val) {
+  MemoryWrite(cur_thread(), CALLERPC, (uptr)addr, val, kSizeLog8);
 }
 
 void __tsan_read1_pc(void *addr, void *pc) {
@@ -66,20 +66,20 @@ void __tsan_read8_pc(void *addr, void *pc) {
   MemoryRead(cur_thread(), (uptr)pc, (uptr)addr, kSizeLog8);
 }
 
-void __tsan_write1_pc(void *addr, void *pc) {
-  MemoryWrite(cur_thread(), (uptr)pc, (uptr)addr, kSizeLog1);
+void __tsan_write1_pc(void *addr, uptr val, void *pc) {
+  MemoryWrite(cur_thread(), (uptr)pc, (uptr)addr, val, kSizeLog1);
 }
 
-void __tsan_write2_pc(void *addr, void *pc) {
-  MemoryWrite(cur_thread(), (uptr)pc, (uptr)addr, kSizeLog2);
+void __tsan_write2_pc(void *addr, uptr val, void *pc) {
+  MemoryWrite(cur_thread(), (uptr)pc, (uptr)addr, val, kSizeLog2);
 }
 
-void __tsan_write4_pc(void *addr, void *pc) {
-  MemoryWrite(cur_thread(), (uptr)pc, (uptr)addr, kSizeLog4);
+void __tsan_write4_pc(void *addr, uptr val, void *pc) {
+  MemoryWrite(cur_thread(), (uptr)pc, (uptr)addr, val, kSizeLog4);
 }
 
-void __tsan_write8_pc(void *addr, void *pc) {
-  MemoryWrite(cur_thread(), (uptr)pc, (uptr)addr, kSizeLog8);
+void __tsan_write8_pc(void *addr, uptr val, void *pc) {
+  MemoryWrite(cur_thread(), (uptr)pc, (uptr)addr, val, kSizeLog8);
 }
 
 void __tsan_vptr_update(void **vptr_p, void *new_val) {
@@ -87,7 +87,7 @@ void __tsan_vptr_update(void **vptr_p, void *new_val) {
   if (*vptr_p != new_val) {
     ThreadState *thr = cur_thread();
     thr->is_vptr_access = true;
-    MemoryWrite(thr, CALLERPC, (uptr)vptr_p, kSizeLog8);
+    MemoryWrite(thr, CALLERPC, (uptr)vptr_p, -1, kSizeLog8);
     thr->is_vptr_access = false;
   }
 }
diff --git a/lib/tsan/rtl/tsan_platform.h b/lib/tsan/rtl/tsan_platform.h
index 135e160..8b69363 100644
--- a/lib/tsan/rtl/tsan_platform.h
+++ b/lib/tsan/rtl/tsan_platform.h
@@ -43,7 +43,9 @@ C/C++ on linux/x86_64 and freebsd/x86_64
 */
 const uptr kMetaShadowBeg = 0x300000000000ull;
 const uptr kMetaShadowEnd = 0x400000000000ull;
+const uptr kWriteMaskBeg  = kMetaShadowEnd;
 const uptr kTraceMemBeg   = 0x600000000000ull;
+const uptr kWriteMaskEnd  = kTraceMemBeg;
 const uptr kTraceMemEnd   = 0x620000000000ull;
 const uptr kShadowBeg     = 0x020000000000ull;
 const uptr kShadowEnd     = 0x100000000000ull;
diff --git a/lib/tsan/rtl/tsan_platform_linux.cc b/lib/tsan/rtl/tsan_platform_linux.cc
index 1309058..076949c 100644
--- a/lib/tsan/rtl/tsan_platform_linux.cc
+++ b/lib/tsan/rtl/tsan_platform_linux.cc
@@ -201,6 +201,13 @@ static void MapRodata() {
 }
 
 void InitializeShadowMemory() {
+  // Map WriteMasks
+  uptr wmask =
+      (uptr)MmapFixedNoReserve(kWriteMaskBeg, kWriteMaskEnd - kWriteMaskBeg, "writemask");
+  if(wmask != kWriteMaskBeg){
+    Printf("FATAL: cannot map write masks %p ~ %p", kWriteMaskBeg, kWriteMaskEnd);
+    Die();
+  }
   // Map memory shadow.
   uptr shadow =
       (uptr)MmapFixedNoReserve(kShadowBeg, kShadowEnd - kShadowBeg, "shadow");
@@ -310,7 +317,7 @@ static void CheckAndProtect() {
 
   ProtectRange(kLoAppMemEnd, kShadowBeg);
   ProtectRange(kShadowEnd, kMetaShadowBeg);
-  ProtectRange(kMetaShadowEnd, kTraceMemBeg);
+  //ProtectRange(kMetaShadowEnd, kTraceMemBeg);
   // Memory for traces is mapped lazily in MapThreadTrace.
   // Protect the whole range for now, so that user does not map something here.
   ProtectRange(kTraceMemBeg, kTraceMemEnd);
diff --git a/lib/tsan/rtl/tsan_rtl.cc b/lib/tsan/rtl/tsan_rtl.cc
index d494aac..9837ce9 100644
--- a/lib/tsan/rtl/tsan_rtl.cc
+++ b/lib/tsan/rtl/tsan_rtl.cc
@@ -388,6 +388,13 @@ int Finalize(ThreadState *thr) {
 
   ThreadFinalize(thr);
 
+  Printf("ThreadSanitizer: bitmasks:\n");
+  for(uptr i=0; i != ws_nodes_pos; ++i){
+    WriteMask* mask = WriteMask::InsFindMask(ws_nodes[i].pc);
+    if(mask)
+      Printf("%p: %p\n", ws_nodes[i].pc, mask->mask);
+  }
+
   if (ctx->nreported) {
     failed = true;
 #ifndef SANITIZER_GO
@@ -670,24 +677,29 @@ do {
   return;
 }
 
-void UnalignedMemoryAccess(ThreadState *thr, uptr pc, uptr addr,
+void UnalignedMemoryAccess(ThreadState *thr, uptr pc, uptr addr, uptr val,
     int size, bool kAccessIsWrite, bool kIsAtomic) {
   while (size) {
     int size1 = 1;
+    uptr val1 = (u8)val;
     int kAccessSizeLog = kSizeLog1;
     if (size >= 8 && (addr & ~7) == ((addr + 7) & ~7)) {
       size1 = 8;
+      val1 = (u64)val;
       kAccessSizeLog = kSizeLog8;
     } else if (size >= 4 && (addr & ~7) == ((addr + 3) & ~7)) {
       size1 = 4;
+      val1 = (u32)val;
       kAccessSizeLog = kSizeLog4;
     } else if (size >= 2 && (addr & ~7) == ((addr + 1) & ~7)) {
       size1 = 2;
+      val1 = (u16)val;
       kAccessSizeLog = kSizeLog2;
     }
-    MemoryAccess(thr, pc, addr, kAccessSizeLog, kAccessIsWrite, kIsAtomic);
+    MemoryAccess(thr, pc, addr, val1, kAccessSizeLog, kAccessIsWrite, kIsAtomic);
     addr += size1;
     size -= size1;
+    val >>= size1;
   }
 }
 
@@ -789,8 +801,27 @@ bool ContainsSameAccess(u64 *s, u64 a, u64 sync_epoch, bool is_write) {
 #endif
 
 ALWAYS_INLINE USED
-void MemoryAccess(ThreadState *thr, uptr pc, uptr addr,
+void MemoryAccess(ThreadState *thr, uptr pc, uptr addr, uptr val,
     int kAccessSizeLog, bool kAccessIsWrite, bool kIsAtomic) {
+  if(kAccessIsWrite && !kIsAtomic){ //FIXME: hasn't implemented atomic write bit mask yet
+    WriteMask* mask = WriteMask::InsCreateMask(pc);
+    uptr new_mask;
+    switch(kAccessSizeLog){
+      case kSizeLog1:
+        new_mask = *(u8*)addr ^ (u8)val;
+        break;
+      case kSizeLog2:
+        new_mask = *(u16*)addr ^ (u16)val;
+        break;
+      case kSizeLog4:
+        new_mask = *(u32*)addr ^ (u32)val;
+        break;
+      case kSizeLog8:
+        new_mask = *(u64*)addr ^ (u64)val;
+        break;
+    }
+    mask->mask |= new_mask;
+  }
   u64 *shadow_mem = (u64*)MemToShadow(addr);
   DPrintf2("#%d: MemoryAccess: @%p %p size=%d"
       " is_write=%d shadow_mem=%p {%zx, %zx, %zx, %zx}\n",
diff --git a/lib/tsan/rtl/tsan_rtl.h b/lib/tsan/rtl/tsan_rtl.h
index a13e4b6..2ad0eba 100644
--- a/lib/tsan/rtl/tsan_rtl.h
+++ b/lib/tsan/rtl/tsan_rtl.h
@@ -34,6 +34,7 @@
 #include "sanitizer_common/sanitizer_libignore.h"
 #include "sanitizer_common/sanitizer_suppressions.h"
 #include "sanitizer_common/sanitizer_thread_registry.h"
+#include "rbtree.h"
 #include "tsan_clock.h"
 #include "tsan_defs.h"
 #include "tsan_flags.h"
@@ -313,6 +314,53 @@ class Shadow : public FastState {
   }
 };
 
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wpedantic"
+
+struct WriteMask{
+    uptr mask;
+    struct{
+        uptr pc : sizeof(uptr) * 8 -1;
+        uptr racy : 1;
+    };
+    static constexpr WriteMask* Beg = (WriteMask*)kWriteMaskBeg;
+    static constexpr WriteMask* End = (WriteMask*)kWriteMaskEnd;
+    ALWAYS_INLINE
+    static WriteMask* InsCreateMask(uptr pc){
+      uptr ith;
+      for(ith = pc % (uptr)(End - Beg);
+          Beg[ith].pc && Beg[ith].pc != pc;
+          ith = (ith + 1) % (uptr)(End - Beg))
+      {} //FIXME: possible infinite loop
+      Beg[ith].pc = pc;
+      return Beg + ith;
+    }
+
+    ALWAYS_INLINE
+    static WriteMask* InsFindMask(uptr pc){
+      uptr ith;
+      for(ith = pc % (uptr)(End - Beg);
+          Beg[ith].pc && Beg[ith].pc != pc;
+          ith = (ith + 1) % (uptr)(End - Beg))
+      {} //FIXME: possible infinite loop
+      if(!Beg[ith].pc){
+        return nullptr;
+      }
+      return Beg + ith;
+    }
+};
+
+#pragma GCC diagnostic pop
+
+struct WriteSetNode{
+    rb_node rbn;
+    uptr pc;
+};
+
+const unsigned long ws_nodes_number = 0x1000ULL;
+extern uptr ws_nodes_pos;
+extern WriteSetNode ws_nodes[ws_nodes_number];
+
 struct ThreadSignalContext;
 
 struct JmpBuf {
@@ -615,7 +663,7 @@ int Finalize(ThreadState *thr);
 void OnUserAlloc(ThreadState *thr, uptr pc, uptr p, uptr sz, bool write);
 void OnUserFree(ThreadState *thr, uptr pc, uptr p, bool write);
 
-void MemoryAccess(ThreadState *thr, uptr pc, uptr addr,
+void MemoryAccess(ThreadState *thr, uptr pc, uptr addr, uptr val,
     int kAccessSizeLog, bool kAccessIsWrite, bool kIsAtomic);
 void MemoryAccessImpl(ThreadState *thr, uptr addr,
     int kAccessSizeLog, bool kAccessIsWrite, bool kIsAtomic,
@@ -624,7 +672,7 @@ void MemoryAccessRange(ThreadState *thr, uptr pc, uptr addr,
     uptr size, bool is_write);
 void MemoryAccessRangeStep(ThreadState *thr, uptr pc, uptr addr,
     uptr size, uptr step, bool is_write);
-void UnalignedMemoryAccess(ThreadState *thr, uptr pc, uptr addr,
+void UnalignedMemoryAccess(ThreadState *thr, uptr pc, uptr addr, uptr val,
     int size, bool kAccessIsWrite, bool kIsAtomic);
 
 const int kSizeLog1 = 0;
@@ -634,22 +682,22 @@ const int kSizeLog8 = 3;
 
 void ALWAYS_INLINE MemoryRead(ThreadState *thr, uptr pc,
                                      uptr addr, int kAccessSizeLog) {
-  MemoryAccess(thr, pc, addr, kAccessSizeLog, false, false);
+  MemoryAccess(thr, pc, addr, 0, kAccessSizeLog, false, false);
 }
 
 void ALWAYS_INLINE MemoryWrite(ThreadState *thr, uptr pc,
-                                      uptr addr, int kAccessSizeLog) {
-  MemoryAccess(thr, pc, addr, kAccessSizeLog, true, false);
+                                      uptr addr, uptr val, int kAccessSizeLog) {
+  MemoryAccess(thr, pc, addr, val, kAccessSizeLog, true, false);
 }
 
 void ALWAYS_INLINE MemoryReadAtomic(ThreadState *thr, uptr pc,
                                            uptr addr, int kAccessSizeLog) {
-  MemoryAccess(thr, pc, addr, kAccessSizeLog, false, true);
+  MemoryAccess(thr, pc, addr, 0, kAccessSizeLog, false, true);
 }
 
 void ALWAYS_INLINE MemoryWriteAtomic(ThreadState *thr, uptr pc,
                                             uptr addr, int kAccessSizeLog) {
-  MemoryAccess(thr, pc, addr, kAccessSizeLog, true, true);
+  MemoryAccess(thr, pc, addr, 0, kAccessSizeLog, true, true);
 }
 
 void MemoryResetRange(ThreadState *thr, uptr pc, uptr addr, uptr size);
diff --git a/lib/tsan/rtl/tsan_rtl_mutex.cc b/lib/tsan/rtl/tsan_rtl_mutex.cc
index 09180d8..1f2bb75 100644
--- a/lib/tsan/rtl/tsan_rtl_mutex.cc
+++ b/lib/tsan/rtl/tsan_rtl_mutex.cc
@@ -69,7 +69,7 @@ void MutexCreate(ThreadState *thr, uptr pc, uptr addr,
   if (!linker_init && IsAppMem(addr)) {
     CHECK(!thr->is_freeing);
     thr->is_freeing = true;
-    MemoryWrite(thr, pc, addr, kSizeLog1);
+    MemoryWrite(thr, pc, addr, -1, kSizeLog1);
     thr->is_freeing = false;
   }
   SyncVar *s = ctx->metamap.GetOrCreateAndLock(thr, pc, addr, true);
@@ -93,7 +93,7 @@ void MutexDestroy(ThreadState *thr, uptr pc, uptr addr) {
   if (IsAppMem(addr)) {
     CHECK(!thr->is_freeing);
     thr->is_freeing = true;
-    MemoryWrite(thr, pc, addr, kSizeLog1);
+    MemoryWrite(thr, pc, addr, -1, kSizeLog1);
     thr->is_freeing = false;
   }
   SyncVar *s = ctx->metamap.GetIfExistsAndLock(addr);
diff --git a/lib/tsan/rtl/tsan_rtl_report.cc b/lib/tsan/rtl/tsan_rtl_report.cc
index dc9438e..e517ee0 100644
--- a/lib/tsan/rtl/tsan_rtl_report.cc
+++ b/lib/tsan/rtl/tsan_rtl_report.cc
@@ -562,6 +562,37 @@ static bool RaceBetweenAtomicAndFree(ThreadState *thr) {
   return false;
 }
 
+uptr ws_nodes_pos = 0;
+WriteSetNode ws_nodes[ws_nodes_number];
+static rb_root WriteSetRoot = RB_ROOT;
+
+static void AddToWriteSet(uptr pc) {
+  struct rb_node **_new = &(WriteSetRoot.rb_node), *parent = nullptr;
+  WriteSetNode *wsn;
+  while (*_new) {
+    wsn = (WriteSetNode *) *_new;
+    parent = *_new;
+    if (wsn->pc > pc)
+      _new = &((*_new)->rb_left);
+    else if (wsn->pc < pc)
+      _new = &((*_new)->rb_right);
+    else
+      return;
+  }
+  if (ws_nodes_pos < ws_nodes_number) {
+    wsn = ws_nodes + ws_nodes_pos++;
+    wsn->pc = pc;
+    rb_link_node(&wsn->rbn, parent, _new);
+    rb_insert_color(&wsn->rbn, &WriteSetRoot);
+  }
+}
+
+void AddStackTopPc(StackTrace* st){
+  if(st->size){
+    AddToWriteSet(st->trace[st->size-1]);
+  }
+}
+
 void ReportRace(ThreadState *thr) {
   CheckNoLocks(thr);
 
@@ -611,12 +642,18 @@ void ReportRace(ThreadState *thr) {
   VarSizeStackTrace traces[kMop];
   const uptr toppc = TraceTopPC(thr);
   ObtainCurrentStack(thr, toppc, &traces[0]);
+  if(Shadow(thr->racy_state[0]).IsWrite()){
+    AddStackTopPc(&traces[0]);
+  }
   if (IsFiredSuppression(ctx, rep, traces[0]))
     return;
   InternalScopedBuffer<MutexSet> mset2(1);
   new(mset2.data()) MutexSet();
   Shadow s2(thr->racy_state[1]);
   RestoreStack(s2.tid(), s2.epoch(), &traces[1], mset2.data());
+  if(Shadow(thr->racy_state[1]).IsWrite()){
+    AddStackTopPc(&traces[1]);
+  }
   if (IsFiredSuppression(ctx, rep, traces[1]))
     return;
 
-- 
1.9.1

